{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis Notebook"
      ],
      "metadata": {
        "id": "1MRzHDgZgJ_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This notebook contains the final implementation for sentiment analysis using various machine learning models. The process involves data preprocessing, feature extraction, model training, hyperparameter tuning, and evaluation.\n",
        "\n",
        "* In here used relabeled dataset & its contain more than `10000` data."
      ],
      "metadata": {
        "id": "55Jn6MlCgPsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "GQFN0MLQgbwQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5ve456qIhs_"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from scipy.sparse import csr_matrix,hstack\n",
        "\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading\n"
      ],
      "metadata": {
        "id": "nuAqdkRsgeXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_train = pd.read_excel('student_feedback_train.xlsx',index_col=False)\n",
        "synthetic_train\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "eDA7rb2BI8Lk",
        "outputId": "baee774b-ee68-4f58-9c57-7bedd48e7c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                sentence   topic  \\\n",
              "0      The cost of this job is very demanding and the...   tutor   \n",
              "1      There are not enough people to listen and list...   tutor   \n",
              "2      The price of this job is so small that it cann...   tutor   \n",
              "3      Mom—I don't know what I'm talking about, I don...   tutor   \n",
              "4      The price of the job is too low for the sales ...   tutor   \n",
              "...                                                  ...     ...   \n",
              "10141                              item practical useful  others   \n",
              "10142         good understanding always evaluating price  others   \n",
              "10143       fish price unreliable price high price range  others   \n",
              "10144               detailed quite bit detailed analysis  others   \n",
              "10145  guy always working creating fake book making n...  others   \n",
              "\n",
              "       Score sentiment  Sentiment Range Sentiment Label  \n",
              "0             0.500000              0.5         Neutral  \n",
              "1             0.500000              0.5         Neutral  \n",
              "2             0.500000              0.5         Neutral  \n",
              "3             0.500000              0.5         Neutral  \n",
              "4             0.500000              0.5         Neutral  \n",
              "...                ...              ...             ...  \n",
              "10141         0.967135              1.0        Positive  \n",
              "10142         0.953008              1.0        Positive  \n",
              "10143         0.802780              0.9        Positive  \n",
              "10144         0.928498              1.0        Positive  \n",
              "10145         0.852216              0.9        Positive  \n",
              "\n",
              "[10146 rows x 5 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>topic</th>\n",
              "      <th>Score sentiment</th>\n",
              "      <th>Sentiment Range</th>\n",
              "      <th>Sentiment Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The cost of this job is very demanding and the...</td>\n",
              "      <td>tutor</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>There are not enough people to listen and list...</td>\n",
              "      <td>tutor</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The price of this job is so small that it cann...</td>\n",
              "      <td>tutor</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mom—I don't know what I'm talking about, I don...</td>\n",
              "      <td>tutor</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The price of the job is too low for the sales ...</td>\n",
              "      <td>tutor</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10141</th>\n",
              "      <td>item practical useful</td>\n",
              "      <td>others</td>\n",
              "      <td>0.967135</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10142</th>\n",
              "      <td>good understanding always evaluating price</td>\n",
              "      <td>others</td>\n",
              "      <td>0.953008</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10143</th>\n",
              "      <td>fish price unreliable price high price range</td>\n",
              "      <td>others</td>\n",
              "      <td>0.802780</td>\n",
              "      <td>0.9</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10144</th>\n",
              "      <td>detailed quite bit detailed analysis</td>\n",
              "      <td>others</td>\n",
              "      <td>0.928498</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10145</th>\n",
              "      <td>guy always working creating fake book making n...</td>\n",
              "      <td>others</td>\n",
              "      <td>0.852216</td>\n",
              "      <td>0.9</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10146 rows × 5 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration"
      ],
      "metadata": {
        "id": "LFUfgrLMl0qH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_train['Sentiment Label'].hist(bins=20, figsize=(4,3), color='r', edgecolor='black')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "73_a3ec4l3T8",
        "outputId": "36ef0e90-453c-4a2f-8d8a-acd697475e84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x300 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets get the length of the mesages\n",
        "length_of_sentence = synthetic_train['sentence'].str.len()\n",
        "length_of_sentence.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG2nGTJmmC2U",
        "outputId": "4961e781-ac9f-4c41-baca-abb56ed5a529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    10146.000000\n",
              "mean         6.013897\n",
              "std          2.638739\n",
              "min          0.000000\n",
              "25%          4.000000\n",
              "50%          6.000000\n",
              "75%          7.000000\n",
              "max         20.000000\n",
              "Name: sentence, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Processing"
      ],
      "metadata": {
        "id": "sMs8-ZiJgp8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cleaning Text Data\n"
      ],
      "metadata": {
        "id": "SzaPEGRlgw_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Remove non-ASCII characters\n",
        "    text = re.sub(r'[^\\x00-\\x7F]', '', text)\n",
        "    # Remove special symbols except for specific punctuation\n",
        "    text = re.sub(r'[^\\w\\s,.!?]', '', text)\n",
        "    # Remove links (URLs)\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Remove leading and trailing whitespace\n",
        "    text = text.strip()\n",
        "    return text.lower()"
      ],
      "metadata": {
        "id": "baf3N1jeJZ2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function to the column\n",
        "synthetic_train['sentence'] = synthetic_train['sentence'].apply(clean_text)"
      ],
      "metadata": {
        "id": "xXR71FtrKB_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing contractions\n",
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozWRb6-9KFAT",
        "outputId": "e09f2e96-8011-4bd7-a0c9-3c07534daf37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in d:\\assignment projects\\sentiment analysis\\env\\lib\\site-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in d:\\assignment projects\\sentiment analysis\\env\\lib\\site-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in d:\\assignment projects\\sentiment analysis\\env\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in d:\\assignment projects\\sentiment analysis\\env\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label Encoding"
      ],
      "metadata": {
        "id": "uBSi9u7Ig1eR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the 'sentiment' column in the training DataFrame\n",
        "synthetic_train['sentiment_label_no'] = label_encoder.fit_transform(synthetic_train['Sentiment Label'])\n",
        "synthetic_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "egIbtCyjKNPF",
        "outputId": "9ef5698d-41e3-4ee7-dea0-f7a1b09f44a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                sentence   topic  \\\n",
              "0      the cost of this job is very demanding and the...   tutor   \n",
              "1      there are not enough people to listen and list...   tutor   \n",
              "2      the price of this job is so small that it cann...   tutor   \n",
              "3      momi dont know what im talking about, i dont k...   tutor   \n",
              "4      the price of the job is too low for the sales ...   tutor   \n",
              "...                                                  ...     ...   \n",
              "10141                              item practical useful  others   \n",
              "10142         good understanding always evaluating price  others   \n",
              "10143       fish price unreliable price high price range  others   \n",
              "10144               detailed quite bit detailed analysis  others   \n",
              "10145  guy always working creating fake book making n...  others   \n",
              "\n",
              "       Score sentiment  Sentiment Range Sentiment Label  sentiment_label_no  \n",
              "0             0.500000              0.5         Neutral                   1  \n",
              "1             0.500000              0.5         Neutral                   1  \n",
              "2             0.500000              0.5         Neutral                   1  \n",
              "3             0.500000              0.5         Neutral                   1  \n",
              "4             0.500000              0.5         Neutral                   1  \n",
              "...                ...              ...             ...                 ...  \n",
              "10141         0.967135              1.0        Positive                   2  \n",
              "10142         0.953008              1.0        Positive                   2  \n",
              "10143         0.802780              0.9        Positive                   2  \n",
              "10144         0.928498              1.0        Positive                   2  \n",
              "10145         0.852216              0.9        Positive                   2  \n",
              "\n",
              "[10146 rows x 6 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>topic</th>\n",
              "      <th>Score sentiment</th>\n",
              "      <th>Sentiment Range</th>\n",
              "      <th>Sentiment Label</th>\n",
              "      <th>sentiment_label_no</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the cost of this job is very demanding and the...</td>\n",
              "      <td>tutor</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>there are not enough people to listen and list...</td>\n",
              "      <td>tutor</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the price of this job is so small that it cann...</td>\n",
              "      <td>tutor</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>momi dont know what im talking about, i dont k...</td>\n",
              "      <td>tutor</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>the price of the job is too low for the sales ...</td>\n",
              "      <td>tutor</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10141</th>\n",
              "      <td>item practical useful</td>\n",
              "      <td>others</td>\n",
              "      <td>0.967135</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10142</th>\n",
              "      <td>good understanding always evaluating price</td>\n",
              "      <td>others</td>\n",
              "      <td>0.953008</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10143</th>\n",
              "      <td>fish price unreliable price high price range</td>\n",
              "      <td>others</td>\n",
              "      <td>0.802780</td>\n",
              "      <td>0.9</td>\n",
              "      <td>Positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10144</th>\n",
              "      <td>detailed quite bit detailed analysis</td>\n",
              "      <td>others</td>\n",
              "      <td>0.928498</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10145</th>\n",
              "      <td>guy always working creating fake book making n...</td>\n",
              "      <td>others</td>\n",
              "      <td>0.852216</td>\n",
              "      <td>0.9</td>\n",
              "      <td>Positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10146 rows × 6 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_train['Sentiment Label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uoaHQ4HTmZ3",
        "outputId": "465572b5-08ed-49a2-9303-cd6d89e1e0bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment Label\n",
              "Positive    4479\n",
              "Negative    3359\n",
              "Neutral     2308\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7CSdZwYgg7x_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Cleaning with NLTK\n",
        "\n"
      ],
      "metadata": {
        "id": "D3dp_0rXg8ID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import contractions\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data files if not already downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkQEreynKN4W",
        "outputId": "49c0a1c7-26ee-40ea-c99d-bb372cce31f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization, Stop Words Removal, and Lemmatization"
      ],
      "metadata": {
        "id": "807p7ZBxhLcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def message_cleaning(message):\n",
        "    # Initialize the WordNet Lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Expand contractions\n",
        "    message = contractions.fix(message)\n",
        "\n",
        "    # Remove punctuation\n",
        "    punc_removed = [char for char in message if char not in string.punctuation]\n",
        "    punc_removed_join = ''.join(punc_removed)\n",
        "\n",
        "    # Tokenize the message\n",
        "    tokens = word_tokenize(punc_removed_join)\n",
        "\n",
        "    # Remove stop words and apply lemmatization\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    cleaned_message = [lemmatizer.lemmatize(word.lower()) for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    return cleaned_message"
      ],
      "metadata": {
        "id": "7-UhsN11Kw_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def message_cleaning_deep(message):\n",
        "    # Initialize the WordNet Lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Expand contractions\n",
        "    message = contractions.fix(message)\n",
        "\n",
        "    # Remove punctuation\n",
        "    punc_removed = [char for char in message if char not in string.punctuation]\n",
        "    punc_removed_join = ''.join(punc_removed)\n",
        "\n",
        "    # Tokenize the message\n",
        "    tokens = word_tokenize(punc_removed_join)\n",
        "\n",
        "    # Remove stop words and apply lemmatization\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    cleaned_words = [lemmatizer.lemmatize(word.lower()) for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # Join the cleaned words back into a single string\n",
        "    cleaned_message = ' '.join(cleaned_words)\n",
        "\n",
        "    return cleaned_message"
      ],
      "metadata": {
        "id": "XLwZkn-iLn4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_train_deep_learining = synthetic_train.copy()\n",
        "synthetic_train_deep_learining['sentence'] = synthetic_train_deep_learining['sentence'].apply(message_cleaning_deep)"
      ],
      "metadata": {
        "id": "A0QLRGiRLfgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_train['sentence'] = synthetic_train['sentence'].apply(message_cleaning)"
      ],
      "metadata": {
        "id": "n-aR9JFYLjSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "_RMJ62XjLtlE",
        "outputId": "00238f88-0648-42df-acfd-8ae7473324b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                sentence   topic  \\\n",
              "0            [cost, job, demanding, student, able, cope]   tutor   \n",
              "1      [enough, people, listen, listen, opinion, stud...   tutor   \n",
              "2             [price, job, small, changed, individually]   tutor   \n",
              "3                    [momus, know, talking, know, going]   tutor   \n",
              "4      [price, job, low, sale, person, student, affec...   tutor   \n",
              "...                                                  ...     ...   \n",
              "10141                          [item, practical, useful]  others   \n",
              "10142   [good, understanding, always, evaluating, price]  others   \n",
              "10143  [fish, price, unreliable, price, high, price, ...  others   \n",
              "10144         [detailed, quite, bit, detailed, analysis]  others   \n",
              "10145  [guy, always, working, creating, fake, book, m...  others   \n",
              "\n",
              "       Score sentiment  Sentiment Range Sentiment Label  sentiment_label_no  \n",
              "0             0.500000              0.5         Neutral                   1  \n",
              "1             0.500000              0.5         Neutral                   1  \n",
              "2             0.500000              0.5         Neutral                   1  \n",
              "3             0.500000              0.5         Neutral                   1  \n",
              "4             0.500000              0.5         Neutral                   1  \n",
              "...                ...              ...             ...                 ...  \n",
              "10141         0.967135              1.0        Positive                   2  \n",
              "10142         0.953008              1.0        Positive                   2  \n",
              "10143         0.802780              0.9        Positive                   2  \n",
              "10144         0.928498              1.0        Positive                   2  \n",
              "10145         0.852216              0.9        Positive                   2  \n",
              "\n",
              "[10146 rows x 6 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>topic</th>\n",
              "      <th>Score sentiment</th>\n",
              "      <th>Sentiment Range</th>\n",
              "      <th>Sentiment Label</th>\n",
              "      <th>sentiment_label_no</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[cost, job, demanding, student, able, cope]</td>\n",
              "      <td>tutor</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[enough, people, listen, listen, opinion, stud...</td>\n",
              "      <td>tutor</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[price, job, small, changed, individually]</td>\n",
              "      <td>tutor</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[momus, know, talking, know, going]</td>\n",
              "      <td>tutor</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[price, job, low, sale, person, student, affec...</td>\n",
              "      <td>tutor</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10141</th>\n",
              "      <td>[item, practical, useful]</td>\n",
              "      <td>others</td>\n",
              "      <td>0.967135</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10142</th>\n",
              "      <td>[good, understanding, always, evaluating, price]</td>\n",
              "      <td>others</td>\n",
              "      <td>0.953008</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10143</th>\n",
              "      <td>[fish, price, unreliable, price, high, price, ...</td>\n",
              "      <td>others</td>\n",
              "      <td>0.802780</td>\n",
              "      <td>0.9</td>\n",
              "      <td>Positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10144</th>\n",
              "      <td>[detailed, quite, bit, detailed, analysis]</td>\n",
              "      <td>others</td>\n",
              "      <td>0.928498</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10145</th>\n",
              "      <td>[guy, always, working, creating, fake, book, m...</td>\n",
              "      <td>others</td>\n",
              "      <td>0.852216</td>\n",
              "      <td>0.9</td>\n",
              "      <td>Positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10146 rows × 6 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering\n",
        "\n",
        "###one hot encoding"
      ],
      "metadata": {
        "id": "phWcNGDUhYyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide topic using one hot encoding\n",
        "synthetic_train = pd.get_dummies(synthetic_train, columns=['topic'], prefix='', prefix_sep='')"
      ],
      "metadata": {
        "id": "3pbzDJfeLu1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc3O4j3RL0nh",
        "outputId": "868b4010-243f-4b9d-b472-85983892106a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10146, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF Vectorization\n"
      ],
      "metadata": {
        "id": "Epvyh9Hbhidg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text_data(train_df, min_df=3):\n",
        "    \"\"\"\n",
        "    Applies TF-IDF vectorization to the text data from training and test DataFrames.\n",
        "\n",
        "    Parameters:\n",
        "    - train_df: pandas DataFrame containing the training data with a text column.\n",
        "    - test_df: pandas DataFrame containing the test data with a text column.\n",
        "    - min_df: int, minimum document frequency for the TF-IDF vectorizer.\n",
        "\n",
        "    Returns:\n",
        "    - X_tfidf_train: sparse matrix, TF-IDF features for the training data.\n",
        "    - X_tfidf_test: sparse matrix, TF-IDF features for the test data.\n",
        "    \"\"\"\n",
        "    # Initialize the TF-IDF Vectorizer\n",
        "    tfidf_vectorizer = TfidfVectorizer(min_df=min_df,ngram_range=(1, 3))\n",
        "\n",
        "    # Convert the text column to a space-separated string if it's a list of tokens\n",
        "    train_text = train_df['sentence'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "\n",
        "    # Fit and transform the training data\n",
        "    X_tfidf_train = tfidf_vectorizer.fit_transform(train_text)\n",
        "\n",
        "    # Transform the test data\n",
        "\n",
        "    return X_tfidf_train\n"
      ],
      "metadata": {
        "id": "t4SGzMgdL2B3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `synthetic_train` and `synthetic_test` are your DataFrames\n",
        "X_tfidf_train = vectorize_text_data(\n",
        "    synthetic_train, min_df=5\n",
        ")"
      ],
      "metadata": {
        "id": "-7p5RMVvMAos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_features(text_features, additional_features):\n",
        "    \"\"\"\n",
        "    Combines text features with additional features into a single feature matrix.\n",
        "\n",
        "    Parameters:\n",
        "    - text_features: sparse matrix, TF-IDF features for the text data.\n",
        "    - additional_features: pandas DataFrame containing the additional features.\n",
        "\n",
        "    Returns:\n",
        "    - combined_features: sparse matrix, combined features including text and additional features.\n",
        "    \"\"\"\n",
        "    # Convert additional features to a sparse matrix\n",
        "    additional_features_sparse = csr_matrix(additional_features.values)\n",
        "\n",
        "    # Combine TF-IDF features with additional features\n",
        "    combined_features = hstack([text_features, additional_features_sparse])\n",
        "\n",
        "    return combined_features"
      ],
      "metadata": {
        "id": "jEIsZ6EPMCVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine TF-IDF features with additional features\n",
        "X_train_tfidf_combined = combine_features(X_tfidf_train, synthetic_train[['curriculum', 'facility', 'others', 'tutor']])"
      ],
      "metadata": {
        "id": "-2fwwfYrMU5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tfidf_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BXRYj3cMXRi",
        "outputId": "0e2d3613-6c08-4e6e-f032-72d5b830fe8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10146, 2698)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {
        "id": "yRz1pjQSiRxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "y_train = synthetic_train['sentiment_label_no']\n",
        "# Assuming y_train is your target variable for training\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf_combined, y_train, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "lmCxy98iMZ_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'SVM': SVC(),\n",
        "    'Naive Bayes': MultinomialNB(),\n",
        "    'XGBoost': XGBClassifier()\n",
        "}"
      ],
      "metadata": {
        "id": "8GmxxanIMzpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Train and evaluate the given model.\n",
        "\n",
        "    Parameters:\n",
        "    - model: an instance of a scikit-learn or XGBoost classifier\n",
        "    - X_train: the training feature matrix\n",
        "    - y_train: the training labels\n",
        "    - X_test: the test feature matrix\n",
        "    - y_test: the test labels\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    # Evaluate the model\n",
        "    print(f\"Model: {model.__class__.__name__}\")\n",
        "    print(f\"Testing accuracy {model.score(X_test, y_test)}\")\n",
        "    print(f\"Training accuracy {model.score(X_train, y_pred_train)} \")\n",
        "    print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "Zx07vk3PNRSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Number of models to train\n",
        "total_models = len(models)\n",
        "\n",
        "# Loop through models with a progress bar\n",
        "for i, (name, model) in enumerate(tqdm(models.items(), desc=\"Training models\", total=total_models), start=1):\n",
        "    print(f\"\\nTraining and evaluating {name}... ({i}/{total_models})\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train and evaluate the model\n",
        "    train_and_evaluate_model(model, X_train, y_train, X_test, y_test)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"{name} training and evaluation took {elapsed_time:.2f} seconds.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmcOT2EbNVtX",
        "outputId": "a7e69f4a-522c-4e9b-af29-fb39ab72a5ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining models:   0%|                                                                           | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training and evaluating Logistic Regression... (1/5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining models:  20%|█████████████▍                                                     | 1/5 [00:00<00:02,  1.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: LogisticRegression\n",
            "Testing accuracy 0.6679802955665025\n",
            "Training accuracy 1.0 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.58      0.62       687\n",
            "           1       0.64      0.58      0.61       464\n",
            "           2       0.68      0.78      0.73       879\n",
            "\n",
            "    accuracy                           0.67      2030\n",
            "   macro avg       0.66      0.65      0.65      2030\n",
            "weighted avg       0.67      0.67      0.66      2030\n",
            "\n",
            "Logistic Regression training and evaluation took 0.56 seconds.\n",
            "\n",
            "Training and evaluating Random Forest... (2/5)\n",
            "Model: RandomForestClassifier\n",
            "Testing accuracy 0.6295566502463055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining models:  40%|██████████████████████████▊                                        | 2/5 [00:27<00:48, 16.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy 1.0 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.52      0.59       687\n",
            "           1       0.57      0.50      0.54       464\n",
            "           2       0.63      0.78      0.70       879\n",
            "\n",
            "    accuracy                           0.63      2030\n",
            "   macro avg       0.63      0.60      0.61      2030\n",
            "weighted avg       0.63      0.63      0.62      2030\n",
            "\n",
            "Random Forest training and evaluation took 27.28 seconds.\n",
            "\n",
            "Training and evaluating SVM... (3/5)\n",
            "Model: SVC\n",
            "Testing accuracy 0.6369458128078818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining models:  60%|████████████████████████████████████████▏                          | 3/5 [01:29<01:14, 37.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy 1.0 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.54      0.60       687\n",
            "           1       0.61      0.46      0.53       464\n",
            "           2       0.63      0.80      0.70       879\n",
            "\n",
            "    accuracy                           0.64      2030\n",
            "   macro avg       0.64      0.60      0.61      2030\n",
            "weighted avg       0.64      0.64      0.63      2030\n",
            "\n",
            "SVM training and evaluation took 61.74 seconds.\n",
            "\n",
            "Training and evaluating Naive Bayes... (4/5)\n",
            "Model: MultinomialNB\n",
            "Testing accuracy 0.6280788177339901\n",
            "Training accuracy 1.0 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.57      0.60       687\n",
            "           1       0.62      0.39      0.48       464\n",
            "           2       0.63      0.80      0.70       879\n",
            "\n",
            "    accuracy                           0.63      2030\n",
            "   macro avg       0.63      0.59      0.59      2030\n",
            "weighted avg       0.63      0.63      0.62      2030\n",
            "\n",
            "Naive Bayes training and evaluation took 0.04 seconds.\n",
            "\n",
            "Training and evaluating XGBoost... (5/5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training models: 100%|███████████████████████████████████████████████████████████████████| 5/5 [01:39<00:00, 19.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: XGBClassifier\n",
            "Testing accuracy 0.6482758620689655\n",
            "Training accuracy 1.0 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.52      0.60       687\n",
            "           1       0.60      0.57      0.59       464\n",
            "           2       0.64      0.79      0.70       879\n",
            "\n",
            "    accuracy                           0.65      2030\n",
            "   macro avg       0.65      0.63      0.63      2030\n",
            "weighted avg       0.65      0.65      0.64      2030\n",
            "\n",
            "XGBoost training and evaluation took 9.47 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyper Parameter Tuning"
      ],
      "metadata": {
        "id": "uDweEETdiZOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from tqdm import tqdm\n",
        "\n",
        "import time"
      ],
      "metadata": {
        "id": "Ce4T-PooNZbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Expanded hyperparameter grids for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],  # Simplified range\n",
        "        'solver': ['liblinear', 'lbfgs', 'saga'],  # Commonly used solvers\n",
        "        'penalty': ['l2'],  # Common penalty\n",
        "        'max_iter': [100, 500, 1000]  # Sufficient for most cases\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [100, 200, 300, 400, 500],\n",
        "        'max_depth': [None, 10, 20, 30, 40, 50],\n",
        "        'min_samples_split': [2, 5, 10, 15],\n",
        "        'min_samples_leaf': [1, 2, 4, 6],\n",
        "        'bootstrap': [True, False],\n",
        "        'max_features': ['auto', 'sqrt', 'log2']\n",
        "    },\n",
        "    'SVM': {\n",
        "        'C': [0.01, 0.1, 1, 10, 100, 1000],\n",
        "        'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
        "        'gamma': ['scale', 'auto'],\n",
        "        'degree': [2, 3, 4],\n",
        "        'coef0': [0.0, 0.1, 0.5, 1.0]\n",
        "    },\n",
        "    'Naive Bayes': {\n",
        "        'alpha': [0.01, 0.1, 0.5, 1.0, 2.0, 5.0],\n",
        "        'fit_prior': [True, False]\n",
        "    },\n",
        "    'XGBoost':\n",
        "        {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.05, 0.1],\n",
        "        'max_depth': [3, 5],\n",
        "        'subsample': [0.8],\n",
        "        'colsample_bytree': [0.8],\n",
        "        'gamma': [0, 0.1],\n",
        "        'reg_alpha': [0.1],\n",
        "        'reg_lambda': [0.5, 1.0]\n",
        "  }\n",
        "\n",
        "    #   {\n",
        "    #     'n_estimators': [50, 100, 200, 300],\n",
        "    #     'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
        "    #     'max_depth': [3, 5, 7, 9, 11],\n",
        "    #     'subsample': [0.6, 0.8, 1.0],\n",
        "    #     'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "    #     'gamma': [0, 0.1, 0.2, 0.3],\n",
        "    #     'reg_alpha': [0, 0.1, 0.5, 1.0],\n",
        "    #     'reg_lambda': [0.1, 0.5, 1.0, 2.0]\n",
        "    # }\n",
        "}\n"
      ],
      "metadata": {
        "id": "PU6aHXecQSrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform Grid Search\n",
        "def tune_model(model, param_grid, X_train, y_train):\n",
        "    \"\"\"\n",
        "    Tunes a machine learning model using GridSearchCV and returns the best model and parameters.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The machine learning model to be tuned.\n",
        "    - param_grid: The parameter grid for GridSearchCV.\n",
        "    - X_train: The training features.\n",
        "    - y_train: The training labels.\n",
        "\n",
        "    Returns:\n",
        "    - best_model: The best model after tuning.\n",
        "    - best_params: The best parameters found by GridSearchCV.\n",
        "    \"\"\"\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    return grid_search.best_estimator_, grid_search.best_params_"
      ],
      "metadata": {
        "id": "2HGq-lZ9QTIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_tune_model(model_name, model, param_grids, X_train, y_train, best_models):\n",
        "    \"\"\"\n",
        "    Tunes and logs the best model and parameters for a given model.\n",
        "\n",
        "    Parameters:\n",
        "    - model_name: The name of the model for logging purposes.\n",
        "    - model: The machine learning model to be tuned.\n",
        "    - param_grids: Dictionary containing hyperparameter grids for each model.\n",
        "    - X_train: The training features.\n",
        "    - y_train: The training labels.\n",
        "    - best_models: Dictionary to store the best model for each model_name.\n",
        "    \"\"\"\n",
        "    if model_name not in param_grids:\n",
        "        print(f\"Error: Parameter grid for {model_name} not found.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Tuning {model_name}...\")\n",
        "    best_model, best_params = tune_model(model, param_grids[model_name], X_train, y_train)\n",
        "    best_models[model_name] = best_model\n",
        "    print(f\"Best parameters for {model_name}: {best_params}\")"
      ],
      "metadata": {
        "id": "J3Tbi0jRQV3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_models = {}"
      ],
      "metadata": {
        "id": "84fkTz1ZQYiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function for each model individually\n",
        "training_tune_model('Logistic Regression', models['Logistic Regression'], param_grids, X_train, y_train, best_models)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP3d8AEfQaEB",
        "outputId": "ff5ff480-7adc-4332-d572-d41bc66b41b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
            "Best parameters for Logistic Regression: {'C': 1, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Call the function for each model individually\n",
        "training_tune_model('XGBoost', models['XGBoost'], param_grids, X_train, y_train, best_models)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-UQdXiYQz81",
        "outputId": "2bd84678-979a-4975-eb1e-b46dc38593a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning XGBoost...\n",
            "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
            "Best parameters for XGBoost: {'colsample_bytree': 0.8, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'reg_alpha': 0.1, 'reg_lambda': 1.0, 'subsample': 0.8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the tuned models\n",
        "for model_name, best_model in best_models.items():\n",
        "    print(f\"\\nEvaluating {model_name}...\")\n",
        "    train_and_evaluate_model(best_model, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ_SsitlQlHZ",
        "outputId": "23a5335a-eafd-4b2d-a5f4-3f18c0103571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating Logistic Regression...\n",
            "Model: LogisticRegression\n",
            "Testing accuracy 0.6679802955665025\n",
            "Training accuracy 1.0 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.58      0.62       687\n",
            "           1       0.64      0.58      0.61       464\n",
            "           2       0.68      0.78      0.73       879\n",
            "\n",
            "    accuracy                           0.67      2030\n",
            "   macro avg       0.66      0.65      0.65      2030\n",
            "weighted avg       0.67      0.67      0.66      2030\n",
            "\n",
            "\n",
            "Evaluating XGBoost...\n",
            "Model: XGBClassifier\n",
            "Testing accuracy 0.638423645320197\n",
            "Training accuracy 1.0 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.49      0.58       687\n",
            "           1       0.61      0.52      0.56       464\n",
            "           2       0.62      0.81      0.71       879\n",
            "\n",
            "    accuracy                           0.64      2030\n",
            "   macro avg       0.64      0.61      0.62      2030\n",
            "weighted avg       0.65      0.64      0.63      2030\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grids['Logistic Regression']={\n",
        "        'C': [0.5, 1, 2, 5],  # Fine-tuning around the best value\n",
        "        'solver': ['saga'],  # Fixed solver as it's the best from previous search\n",
        "        'penalty': ['l2'],  # Fixed penalty as it's the best from previous search\n",
        "        'max_iter': [250, 500, 750, 1000],  # Fine-tuning around the best value\n",
        "        'tol': [1e-4, 1e-3, 1e-2]  # Added tolerance for convergence criteria\n",
        "    }"
      ],
      "metadata": {
        "id": "XDdEPn3fUalE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function for each model individually\n",
        "training_tune_model('Logistic Regression', models['Logistic Regression'], param_grids, X_train, y_train, best_models)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4XcjC3bfca-",
        "outputId": "70d58e79-472b-467f-9bd5-90ea33cc1a26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Logistic Regression...\n",
            "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
            "Best parameters for Logistic Regression: {'C': 1, 'max_iter': 750, 'penalty': 'l2', 'solver': 'saga', 'tol': 0.01}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the tuned models\n",
        "for model_name, best_model in best_models.items():\n",
        "    print(f\"\\nEvaluating {model_name}...\")\n",
        "    train_and_evaluate_model(best_model, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0KivnlbfhAl",
        "outputId": "756de50a-4b00-419e-c733-7457d3146dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating Logistic Regression...\n",
            "Model: LogisticRegression\n",
            "Testing accuracy 0.6674876847290641\n",
            "Training accuracy 1.0 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.58      0.62       687\n",
            "           1       0.64      0.58      0.61       464\n",
            "           2       0.68      0.79      0.73       879\n",
            "\n",
            "    accuracy                           0.67      2030\n",
            "   macro avg       0.66      0.65      0.65      2030\n",
            "weighted avg       0.67      0.67      0.66      2030\n",
            "\n",
            "\n",
            "Evaluating XGBoost...\n",
            "Model: XGBClassifier\n",
            "Testing accuracy 0.638423645320197\n",
            "Training accuracy 1.0 \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.49      0.58       687\n",
            "           1       0.61      0.52      0.56       464\n",
            "           2       0.62      0.81      0.71       879\n",
            "\n",
            "    accuracy                           0.64      2030\n",
            "   macro avg       0.64      0.61      0.62      2030\n",
            "weighted avg       0.65      0.64      0.63      2030\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZbR7U9VTfmJR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}